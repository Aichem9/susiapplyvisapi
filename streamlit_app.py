# streamlit_app.py
# ëŒ€í•™ ì§€ì› í˜„í™© ì‹œê°í™” + ë‹¤ì¤‘ íŒŒì¼ í•©ì‚° + 'ì¬ìš”ì²­' í–‰ ì œê±° + GPT ë³´ê³ ì„œ ìƒì„±(+ë‹¤ìš´ë¡œë“œ)
# by @ssac9 ìš”ì²­ì‚¬í•­ ë°˜ì˜

import streamlit as st
import pandas as pd
import plotly.express as px
from io import BytesIO
import json

st.set_page_config(page_title="ëŒ€í•™ ì§€ì› í˜„í™© - í†µí•©/ë³´ê³ ì„œ", layout="wide")
st.title("ëŒ€í•™ ì§€ì› í˜„í™© (ë‹¤ì¤‘ íŒŒì¼Â·ë§‰ëŒ€ê·¸ë˜í”„Â·ì»¬ëŸ¬í’€ + GPT ë³´ê³ ì„œ)")

st.markdown("""
**ì‚¬ìš© ì•ˆë‚´**  
- ê°™ì€ ì–‘ì‹ì˜ ì—‘ì…€ íŒŒì¼ì„ **ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œ**í•˜ë©´ **ëª¨ë“  íŒŒì¼ì„ í•©ì‚°**í•´ ëŒ€í•™(Gì—´)ë³„ ì§€ì› ë¹ˆë„ ë§‰ëŒ€ê·¸ë˜í”„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  
- ì—…ë¡œë“œëœ íŒŒì¼ì˜ **ì–´ëŠ ì—´ì—ë“  'ì¬ìš”ì²­'** ì´ë¼ëŠ” ë¬¸êµ¬ê°€ í¬í•¨ëœ **í–‰ì€ ì „ë¶€ ì œì™¸**í•˜ê³  ì§‘ê³„í•©ë‹ˆë‹¤.  
- **ê·¸ë˜í”„ ì œëª©(ë‹¨ì¼ íŒŒì¼ ì—…ë¡œë“œ ì‹œ)**ì€ **C, D, Bì—´** ë°ì´í„°ë¥¼ ì¡°í•©í•´ ìë™ ìƒì„±ë©ë‹ˆë‹¤. ì˜ˆ) `2025í•™ë…„ë„ 3í•™ë…„ 6ë°˜ ìˆ˜ì‹œ ì§€ì› ëŒ€í•™ ì‹œê°í™”`  
- **ë³´ê³ ì„œ ìë™ ì‘ì„±**: ì•„ë˜ **OpenAI API í‚¤**ë¥¼ ì…ë ¥í•˜ë©´ ì§‘ê³„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì¸ì„œìš¸ â†’ ê²½ê¸°ê¶Œ â†’ ì§€ë°©ëŒ€í•™** ìˆœì„œì˜ ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.  
- ê³µë°±/ê²°ì¸¡ ê°’ì€ `"ë¯¸ê¸°ì¬"`ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.  
- ê° ëŒ€í•™ ë§‰ëŒ€ëŠ” **ë‹¤ì±„ë¡œìš´ ìƒ‰ìƒ íŒ”ë ˆíŠ¸**ë¡œ í‘œì‹œë©ë‹ˆë‹¤.  

ğŸ“‚ **ì—‘ì…€ íŒŒì¼ ì €ì¥ ë°©ë²•**  
ğŸ‘‰ **ë‚˜ì´ìŠ¤ > ëŒ€ì…ì „í˜• > ì œê³µí˜„í™© ì¡°íšŒ > ì—‘ì…€íŒŒì¼ë¡œ ì €ì¥**
""")

# ----------------------------- ì…ë ¥ UI -----------------------------
uploaded_files = st.file_uploader(
    "ì—‘ì…€ íŒŒì¼(.xlsx)ì„ í•˜ë‚˜ ì´ìƒ ì—…ë¡œë“œí•˜ì„¸ìš”",
    type=["xlsx"],
    accept_multiple_files=True
)

mapping_file = st.file_uploader(
    "ì„ íƒ: ëŒ€í•™-ê¶Œì—­ ë§¤í•‘ CSV ì—…ë¡œë“œ (ì—´ ì´ë¦„: ëŒ€í•™, ê¶Œì—­ / ê¶Œì—­: ì¸ì„œìš¸Â·ê²½ê¸°ê¶ŒÂ·ì§€ë°©ëŒ€í•™)",
    type=["csv"]
)

with st.expander("GPT ë³´ê³ ì„œ ìƒì„±(ì„ íƒ ì‚¬í•­)"):
    st.caption("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ë©´ ë³´ê³ ì„œë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤. (í‚¤ëŠ” ì„¸ì…˜ ë‚´ì—ì„œë§Œ ì‚¬ìš©)")
    api_key = st.text_input("OpenAI API Key", type="password", placeholder="sk-...")
    model_name = st.text_input("ëª¨ë¸ ì´ë¦„", value="gpt-4o-mini", help="ì˜ˆ: gpt-4o-mini, gpt-4o, gpt-4.1 ë“±")
    generate_btn = st.button("ë³´ê³ ì„œ ìƒì„±")

# ----------------------------- ìœ í‹¸ í•¨ìˆ˜ -----------------------------
def safe_read_excel(file):
    try:
        df = pd.read_excel(file, dtype=str)
        df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)
        return df
    except Exception as e:
        st.error(f"[{getattr(file, 'name', 'íŒŒì¼')}] ì—‘ì…€ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def default_col_by_letter(df, letter):
    pos = ord(letter.upper()) - ord('A') + 1
    if 1 <= pos <= len(df.columns):
        return df.columns[pos-1]
    return None

def remove_rows_with_keyword(df: pd.DataFrame, keyword: str = "ì¬ìš”ì²­"):
    """ì–´ëŠ ì—´ì—ë“  keywordê°€ í¬í•¨ëœ í–‰ì€ ì‚­ì œ."""
    if df is None or df.empty:
        return df, 0
    # ë¬¸ìì—´í™” í›„ í¬í•¨ ì—¬ë¶€ ì²´í¬
    sdf = df.astype(str)
    mask_any = sdf.apply(lambda col: col.str.contains(keyword, na=False)).any(axis=1)
    removed = int(mask_any.sum())
    cleaned = df.loc[~mask_any].copy()
    return cleaned, removed

def build_univ_counts_from_series(series: pd.Series) -> pd.DataFrame:
    s = series.astype(str)
    s = s.replace({"": "ë¯¸ê¸°ì¬", "NaN": "ë¯¸ê¸°ì¬", "nan": "ë¯¸ê¸°ì¬", "None": "ë¯¸ê¸°ì¬"}).fillna("ë¯¸ê¸°ì¬")
    s = s.apply(lambda x: x.strip() if isinstance(x, str) else x)
    vc = s.value_counts(dropna=False)
    out = vc.rename_axis("ëŒ€í•™").reset_index(name="ì§€ì›ìˆ˜")
    out = out.sort_values("ì§€ì›ìˆ˜", ascending=False, kind="mergesort").reset_index(drop=True)
    return out

def make_title_from_df(df):
    try:
        c_val = str(df.iloc[0, 2]) if df.shape[1] > 2 else ""
        d_val = str(df.iloc[0, 3]) if df.shape[1] > 3 else ""
        b_val = str(df.iloc[0, 1]) if df.shape[1] > 1 else ""
        base = " ".join([v for v in [c_val, d_val, b_val] if v])
        if base.strip():
            return f"{base} ìˆ˜ì‹œ ì§€ì› ëŒ€í•™ ì‹œê°í™”"
    except Exception:
        pass
    return "ëŒ€í•™ë³„ ì§€ì› ë¹ˆë„ ì‹œê°í™”"

def build_region_map_from_csv(file) -> dict:
    try:
        df = pd.read_csv(file, dtype=str)
        df = df.rename(columns={c: c.strip() for c in df.columns})
        assert "ëŒ€í•™" in df.columns and "ê¶Œì—­" in df.columns
        mp = {}
        for _, row in df.iterrows():
            u = str(row["ëŒ€í•™"]).strip()
            r = str(row["ê¶Œì—­"]).strip()
            if u and r in ["ì¸ì„œìš¸", "ê²½ê¸°ê¶Œ", "ì§€ë°©ëŒ€í•™"]:
                mp[u] = r
        return mp
    except Exception as e:
        st.warning(f"ë§¤í•‘ CSVë¥¼ ì½ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return {}

# ë‚´ì¥(ê°„ì´) ë§¤í•‘: í•„ìš”ì‹œ CSVë¡œ ë³´ì™„ ê¶Œì¥
BUILTIN_REGION_MAP = {
    # ì¸ì„œìš¸ (ëŒ€í‘œ ì˜ˆì‹œ)
    "ì„œìš¸ëŒ€": "ì¸ì„œìš¸", "ì„œìš¸ëŒ€í•™êµ": "ì¸ì„œìš¸",
    "ì—°ì„¸": "ì¸ì„œìš¸", "ê³ ë ¤": "ì¸ì„œìš¸", "í•œì–‘": "ì¸ì„œìš¸", "ì„±ê· ê´€": "ì¸ì„œìš¸", "ì„œê°•": "ì¸ì„œìš¸",
    "ì¤‘ì•™": "ì¸ì„œìš¸", "ê²½í¬": "ì¸ì„œìš¸", "í•œêµ­ì™¸êµ­ì–´": "ì¸ì„œìš¸", "ì™¸êµ­ì–´": "ì¸ì„œìš¸", "ë™êµ­": "ì¸ì„œìš¸",
    "ê±´êµ­": "ì¸ì„œìš¸", "í™ìµ": "ì¸ì„œìš¸", "ìˆ™ëª…": "ì¸ì„œìš¸", "ì´í™”": "ì¸ì„œìš¸",
    # ê²½ê¸°ê¶Œ (ëŒ€í‘œ ì˜ˆì‹œÂ·ìˆ˜ë„ê¶Œ í¬í•¨)
    "ì•„ì£¼": "ê²½ê¸°ê¶Œ", "ê²½ê¸°ëŒ€": "ê²½ê¸°ê¶Œ", "ë‹¨êµ­": "ê²½ê¸°ê¶Œ", "ìš©ì¸": "ê²½ê¸°ê¶Œ", "ì£½ì „": "ê²½ê¸°ê¶Œ",
    "ê°€ì²œ": "ê²½ê¸°ê¶Œ", "í•œì–‘ëŒ€(ERICA)": "ê²½ê¸°ê¶Œ", "í•œê²½": "ê²½ê¸°ê¶Œ", "ì¸ì²œ": "ê²½ê¸°ê¶Œ", "ì¸í•˜": "ê²½ê¸°ê¶Œ",
}

def heuristic_region(univ_name: str) -> str:
    n = (univ_name or "").strip()
    if n == "" or n == "ë¯¸ê¸°ì¬":
        return "ì§€ë°©ëŒ€í•™"
    # ë‚´ì¥ í‚¤ì›Œë“œ/ëŒ€í•™ëª…
    for key, region in BUILTIN_REGION_MAP.items():
        if key in n:
            return region
    # í‚¤ì›Œë“œ ê¸°ë°˜
    if "ì„œìš¸" in n:
        return "ì¸ì„œìš¸"
    if any(k in n for k in ["ê²½ê¸°", "ìˆ˜ì›", "ìš©ì¸", "ë¶„ë‹¹", "ì„±ë‚¨", "ì•ˆì–‘", "ì˜ì •ë¶€", "ì¸ì²œ", "ìˆ˜ë„ê¶Œ", "ì¼ì‚°", "ê³ ì–‘"]):
        return "ê²½ê¸°ê¶Œ"
    return "ì§€ë°©ëŒ€í•™"

def apply_region(df_counts: pd.DataFrame, mapping: dict) -> pd.DataFrame:
    def _map_one(u):
        if u in mapping:
            return mapping[u]
        for k, v in mapping.items():
            if k and k in u:
                return v
        return heuristic_region(u)
    out = df_counts.copy()
    out["ê¶Œì—­"] = out["ëŒ€í•™"].apply(_map_one)
    return out

def to_bytes_download(data: str, filename: str, mime: str = "text/markdown"):
    bio = BytesIO()
    bio.write(data.encode("utf-8-sig"))
    bio.seek(0)
    st.download_button(
        label=f"{filename} ë‹¤ìš´ë¡œë“œ",
        data=bio,
        file_name=filename,
        mime=mime
    )

# ----------------------------- ë©”ì¸ ì²˜ë¦¬ -----------------------------
if uploaded_files:
    # 1) ì²« íŒŒì¼ ë¡œë“œ(ì»¬ëŸ¼ ì¶”ì •/íƒ€ì´í‹€ ìƒì„±ìš©)
    first_df = safe_read_excel(uploaded_files[0])
    if first_df is None or first_df.empty:
        st.warning("ì²« ë²ˆì§¸ íŒŒì¼ì´ ë¹„ì–´ ìˆê±°ë‚˜ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    # 'ì¬ìš”ì²­' ì œê±°
    first_df_clean, removed_first = remove_rows_with_keyword(first_df, "ì¬ìš”ì²­")

    default_univ_col = default_col_by_letter(first_df_clean, "G") or first_df_clean.columns[0]
    univ_col = st.selectbox(
        "ëŒ€í•™(ë¹ˆë„) ì»¬ëŸ¼ ì„ íƒ (ëª¨ë“  íŒŒì¼ì— ë™ì¼í•˜ê²Œ ì ìš©)",
        options=list(first_df_clean.columns),
        index=(list(first_df_clean.columns).index(default_univ_col) if default_univ_col in first_df_clean.columns else 0),
        help="ë³´í†µ Gì—´(7ë²ˆì§¸ ì—´)ì´ ëŒ€í•™ëª…ì…ë‹ˆë‹¤."
    )

    # ë‹¨ì¼/ë‹¤ì¤‘ íŒŒì¼ì— ë”°ë¥¸ ê·¸ë˜í”„ ì œëª©
    graph_title = make_title_from_df(first_df_clean) if len(uploaded_files) == 1 else "ì „ì²´(ë‹¤ì¤‘ íŒŒì¼) ìˆ˜ì‹œ ì§€ì› ëŒ€í•™ ì‹œê°í™”"

    # 2) ì‚¬ìš©ì ë§¤í•‘ ë¡œë“œ
    user_map = build_region_map_from_csv(mapping_file) if mapping_file is not None else {}

    # 3) ëª¨ë“  íŒŒì¼ ë¡œë“œ + 'ì¬ìš”ì²­' ì œê±° + í•©ì‚°ìš© ì‹œë¦¬ì¦ˆ ëª¨ìŒ
    all_univ_values = []
    per_file_counts = []
    total_removed = removed_first  # ì œê±° ëˆ„ì 

    for f in uploaded_files:
        df = safe_read_excel(f)
        if df is None or df.empty:
            st.warning(f"ë¹„ì–´ ìˆê±°ë‚˜ ì½ì„ ìˆ˜ ì—†ëŠ” íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤: {getattr(f, 'name', 'íŒŒì¼')}")
            continue

        # 'ì¬ìš”ì²­' í–‰ ì œê±°
        df, removed = remove_rows_with_keyword(df, "ì¬ìš”ì²­")
        total_removed += removed

        if univ_col not in df.columns:
            st.warning(f"ì„ íƒí•œ ì»¬ëŸ¼ '{univ_col}'ì´ ì—†ëŠ” íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤: {getattr(f, 'name', 'íŒŒì¼')}")
            continue

        s = df[univ_col]
        all_univ_values.append(s)
        per_file_counts.append({
            "file": getattr(f, "name", "íŒŒì¼"),
            "removed_rows": removed,
            "counts": build_univ_counts_from_series(s)
        })

    if total_removed > 0:
        st.info(f"âš ï¸ 'ì¬ìš”ì²­' ë¬¸êµ¬ê°€ í¬í•¨ëœ í–‰ {total_removed}ê±´ì„ ì œì™¸í•˜ê³  ì§‘ê³„í–ˆìŠµë‹ˆë‹¤.")

    if not all_univ_values:
        st.error("ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì»¬ëŸ¼ ì„ íƒ ë˜ëŠ” íŒŒì¼ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        st.stop()

    # 4) í•©ì‚° ì§‘ê³„
    merged_series = pd.concat(all_univ_values, ignore_index=True)
    total_counts = build_univ_counts_from_series(merged_series)

    # 5) ê¶Œì—­ ë¶€ì—¬ ë° ê¶Œì—­ë³„ í•©ê³„
    total_with_region = apply_region(total_counts, user_map)
    region_order = ["ì¸ì„œìš¸", "ê²½ê¸°ê¶Œ", "ì§€ë°©ëŒ€í•™"]
    region_summary = (
        total_with_region.groupby("ê¶Œì—­")["ì§€ì›ìˆ˜"].sum().reindex(region_order).fillna(0).astype(int).reset_index()
    )

    # 6) ì‹œê°í™” ì˜µì…˜
    c1, c2 = st.columns([1, 3])
    with c1:
        top_n = st.number_input("ìƒìœ„ Nê°œë§Œ í‘œì‹œ (0=ì „ì²´)", min_value=0, max_value=int(len(total_counts)), value=min(20, int(len(total_counts))))
    with c2:
        sort_desc = st.checkbox("ë¹ˆë„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬", value=True)

    plot_df = total_counts.copy()
    if sort_desc:
        plot_df = plot_df.sort_values("ì§€ì›ìˆ˜", ascending=False, kind="mergesort")
    if top_n and top_n > 0:
        plot_df = plot_df.head(int(top_n))

    palette = px.colors.qualitative.Set3 + px.colors.qualitative.Vivid + px.colors.qualitative.Dark24

    fig = px.bar(
        plot_df,
        x="ëŒ€í•™",
        y="ì§€ì›ìˆ˜",
        color="ëŒ€í•™",
        text="ì§€ì›ìˆ˜",
        title=graph_title,
        color_discrete_sequence=palette
    )
    fig.update_traces(textposition="outside")
    fig.update_layout(
        xaxis_tickangle=-45,
        xaxis_title=None,
        yaxis_title=None,
        margin=dict(l=10, r=10, t=60, b=10),
        showlegend=False
    )
    st.plotly_chart(fig, use_container_width=True)

    with st.expander("ì „ì²´ í•©ì‚° í‘œ ë³´ê¸° (ëŒ€í•™ë³„)"):
        st.dataframe(total_counts, use_container_width=True)
        st.download_button(
            "ëŒ€í•™ë³„ í•©ì‚° CSV ë‹¤ìš´ë¡œë“œ",
            data=total_counts.to_csv(index=False).encode("utf-8-sig"),
            file_name="ëŒ€í•™ë³„_ì§€ì›ë¹ˆë„_ì „ì²´í•©ì‚°.csv",
            mime="text/csv"
        )

    with st.expander("ê¶Œì—­ë³„ í•©ê³„ ë³´ê¸°"):
        st.dataframe(region_summary, use_container_width=True)
        st.download_button(
            "ê¶Œì—­ë³„ í•©ê³„ CSV ë‹¤ìš´ë¡œë“œ",
            data=region_summary.to_csv(index=False).encode("utf-8-sig"),
            file_name="ê¶Œì—­ë³„_í•©ê³„.csv",
            mime="text/csv"
        )

    with st.expander("íŒŒì¼ë³„ ì§‘ê³„(ê²€ì¦ìš©)"):
        for item in per_file_counts:
            st.markdown(f"**íŒŒì¼:** {item['file']} (ì œê±°ëœ í–‰: {item['removed_rows']}ê±´)")
            st.dataframe(item["counts"], use_container_width=True)
            st.markdown("---")

    # ----------------------------- GPT ë³´ê³ ì„œ ìƒì„± -----------------------------
    st.subheader("GPT ê¸°ë°˜ ë¶„ì„ ë³´ê³ ì„œ (ì¸ì„œìš¸ â†’ ê²½ê¸°ê¶Œ â†’ ì§€ë°©ëŒ€í•™)")

    # ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ ë°ì´í„° í˜ì´ë¡œë“œ(ê°„ê²° JSON)
    # ê° ê¶Œì—­ì˜ ìƒìœ„ ëŒ€í•™ TOP 10ë„ ì¶”ì¶œ
    def top_univs_by_region(df_regioned: pd.DataFrame, region: str, k=10):
        sub = df_regioned[df_regioned["ê¶Œì—­"] == region][["ëŒ€í•™", "ì§€ì›ìˆ˜"]].sort_values("ì§€ì›ìˆ˜", ascending=False)
        return sub.head(k).to_dict(orient="records")

    payload = {
        "total_by_region": region_summary.to_dict(orient="records"),
        "top_univs": {
            "ì¸ì„œìš¸": top_univs_by_region(total_with_region, "ì¸ì„œìš¸"),
            "ê²½ê¸°ê¶Œ": top_univs_by_region(total_with_region, "ê²½ê¸°ê¶Œ"),
            "ì§€ë°©ëŒ€í•™": top_univs_by_region(total_with_region, "ì§€ë°©ëŒ€í•™"),
        },
        "overall_top": total_counts.head(20).to_dict(orient="records"),  # ì „ì²´ TOP20
    }

    # í”„ë¡¬í”„íŠ¸
    system_prompt = (
        "ë„ˆëŠ” í•œêµ­ ê³ ë“±í•™êµ ì§„í•™ë¶€ êµì‚¬ì—ê²Œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ë°ì´í„° ë¶„ì„ ë¹„ì„œë‹¤. "
        "ì…ë ¥ JSONì„ ë°”íƒ•ìœ¼ë¡œ 'ì¸ì„œìš¸ â†’ ê²½ê¸°ê¶Œ â†’ ì§€ë°©ëŒ€í•™' ìˆœìœ¼ë¡œ ì§€ì› í˜„í™©ì„ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë¶„ì„í•´ë¼. "
        "ìˆ«ìëŠ” í‘œì™€ ë¶ˆë¦¿ì„ ì ì ˆíˆ ì„ê³ , ì˜ë¯¸ ìˆëŠ” ì¸ì‚¬ì´íŠ¸(ì§‘ì¤‘ë„, ë¶„ì‚°ë„, ìƒìœ„ ëŒ€í•™ í´ëŸ¬ìŠ¤í„°, íŠ¹ì§•ì ì¸ ì „ë°˜ ê²½í–¥)ë¥¼ í¬í•¨í•˜ë¼. "
        "ë§ˆì§€ë§‰ì— 'ì§€ë„Â·í–‰ì • ì°¸ê³ ì‚¬í•­' ì„¹ì…˜ìœ¼ë¡œ ì‹¤ë¬´ íŒì„ 3~5ê°œ ì œì‹œí•˜ë¼. "
        "ì¶œë ¥ í˜•ì‹ì€ Markdownìœ¼ë¡œ ì‘ì„±í•œë‹¤."
    )
    user_prompt = (
        "ì•„ë˜ëŠ” ì§‘ê³„ ë°ì´í„°ë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ì„œìš¸, ê²½ê¸°ê¶Œ, ì§€ë°©ëŒ€í•™ ìˆœì„œì˜ ì§€ì› í˜„í™© ë³´ê³ ì„œë¥¼ í•œêµ­ì–´ Markdownìœ¼ë¡œ ì‘ì„±í•´ì¤˜. "
        "ê°€ëŠ¥í•˜ë©´ í‘œ(ê¶Œì—­ë³„ í•©ê³„, ê¶Œì—­ë³„ ìƒìœ„ ëŒ€í•™ TOP)ë¥¼ í¬í•¨í•´ì¤˜.\n\n"
        f"ë°ì´í„°(JSON):\n```json\n{json.dumps(payload, ensure_ascii=False, indent=2)}\n```"
    )

    # ë³´ê³ ì„œ ë¯¸ë¦¬ë³´ê¸° ì˜ì—­
    report_md = st.empty()

    if generate_btn:
        if not api_key:
            st.error("OpenAI API Keyë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        else:
            # OpenAI í˜¸ì¶œ (ìµœì‹ /êµ¬ë²„ì „ ëª¨ë‘ ì‹œë„)
            content = None
            error_msg = None
            try:
                # New-style SDK (openai>=1.0)
                from openai import OpenAI
                client = OpenAI(api_key=api_key)
                resp = client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.2
                )
                content = resp.choices[0].message.content
            except Exception as e_new:
                try:
                    # Legacy fallback
                    import openai
                    openai.api_key = api_key
                    resp = openai.ChatCompletion.create(
                        model=model_name,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        temperature=0.2
                    )
                    content = resp["choices"][0]["message"]["content"]
                except Exception as e_old:
                    error_msg = f"OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {e_new} / {e_old}"

            if error_msg:
                st.error(error_msg)
            else:
                report_md.markdown(content)
                # ë‹¤ìš´ë¡œë“œ(.md, .txt)
                to_bytes_download(content, "ì§€ì›í˜„í™©_ë¶„ì„ë³´ê³ ì„œ.md", mime="text/markdown")
                to_bytes_download(content, "ì§€ì›í˜„í™©_ë¶„ì„ë³´ê³ ì„œ.txt", mime="text/plain")
else:
    st.info("ì—‘ì…€ íŒŒì¼ì„ 1ê°œ ì´ìƒ ì—…ë¡œë“œí•˜ë©´ ì „ì²´ í•©ì‚° ê²°ê³¼ì™€ ë³´ê³ ì„œë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
