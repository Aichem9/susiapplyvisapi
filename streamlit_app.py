import streamlit as st
import pandas as pd
import plotly.express as px
import requests
import json
import time

st.set_page_config(page_title="ëŒ€í•™ ì§€ì› í˜„í™© - ë‹¤ì¤‘ íŒŒì¼ í•©ì‚°", layout="wide")
st.title("ëŒ€ì… ì „í˜•ìë£Œ ì¡°íšŒ ë°ì´í„° ê¸°ë°˜ ì§€ì› í˜„í™© ì‹œê°í™” (ë‹¤ì¤‘ íŒŒì¼Â·ë§‰ëŒ€ê·¸ë˜í”„Â·ì»¬ëŸ¬í’€)")

st.markdown("""
**ì‚¬ìš© ì•ˆë‚´**  
- ê°™ì€ ì–‘ì‹ì˜ ì—‘ì…€ íŒŒì¼ì„ **ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œ**í•˜ë©´ **ëª¨ë“  íŒŒì¼ì„ í•©ì‚°**í•´ ëŒ€í•™(Gì—´)ë³„ ì§€ì› ë¹ˆë„ ë§‰ëŒ€ê·¸ë˜í”„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  
- ê·¸ë˜í”„ ì œëª©ì€ **ë‹¨ì¼ íŒŒì¼ ì—…ë¡œë“œ ì‹œ** C, D, Bì—´(ì˜ˆ: `2025í•™ë…„ë„ 3í•™ë…„ 6ë°˜`)ì„ ì¡°í•©í•´ ìë™ ìƒì„±ë©ë‹ˆë‹¤. **ì—¬ëŸ¬ íŒŒì¼ ì—…ë¡œë“œ ì‹œ**ì—” `ì „ì²´(ë‹¤ì¤‘ íŒŒì¼)`ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.  
- ê³µë°±/ê²°ì¸¡ì€ `"ë¯¸ê¸°ì¬"`ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.  
- **"ì¬ìš”ì²­"ì´ í¬í•¨ëœ í–‰ì˜ ë°ì´í„°ëŠ” ìë™ìœ¼ë¡œ ì œì™¸**ë©ë‹ˆë‹¤.
- ê° ëŒ€í•™ ë§‰ëŒ€ëŠ” **ë‹¤ì±„ë¡œìš´ ìƒ‰ìƒ íŒ”ë ˆíŠ¸**ë¡œ í‘œì‹œë©ë‹ˆë‹¤.  
- **GPT APIë¥¼ í†µí•´ ì§€ì—­ë³„ ëŒ€í•™ ì§€ì› í˜„í™© ë¶„ì„ ë³´ê³ ì„œ**ë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤.
- ì¸ì°½ê³  AIchem ì œì‘ : ssac9@sen.go.kr

ğŸ“‚ **ì—‘ì…€ íŒŒì¼ ì €ì¥ ë°©ë²•**  
ğŸ‘‰ **ë‚˜ì´ìŠ¤ > ëŒ€ì…ì „í˜• > ì œê³µí˜„í™© ì¡°íšŒ > ì—‘ì…€íŒŒì¼ë¡œ ì €ì¥**
""")

def validate_api_key(api_key):
    """
    OpenAI API í‚¤ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # ê°„ë‹¨í•œ API í˜¸ì¶œë¡œ í‚¤ ê²€ì¦ (ëª¨ë¸ ëª©ë¡ ì¡°íšŒ)
        response = requests.get(
            "https://api.openai.com/v1/models",
            headers=headers,
            timeout=10
        )
        
        if response.status_code == 200:
            models_data = response.json()
            # GPT ëª¨ë¸ë“¤ë§Œ í•„í„°ë§
            available_models = []
            for model in models_data.get('data', []):
                model_id = model.get('id', '')
                if 'gpt' in model_id and ('3.5' in model_id or '4' in model_id):
                    available_models.append(model_id)
            
            return {
                "valid": True, 
                "models": available_models[:5],  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                "error": None
            }
        elif response.status_code == 401:
            return {
                "valid": False, 
                "models": [],
                "error": "API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
            }
        elif response.status_code == 429:
            return {
                "valid": False, 
                "models": [],
                "error": "API ì‚¬ìš©ëŸ‰ í•œë„ê°€ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            }
        else:
            return {
                "valid": False, 
                "models": [],
                "error": f"API ì—°ê²° ì˜¤ë¥˜ (ì½”ë“œ: {response.status_code})"
            }
            
    except requests.exceptions.Timeout:
        return {
            "valid": False, 
            "models": [],
            "error": "API ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        }
    except requests.exceptions.RequestException as e:
        return {
            "valid": False, 
            "models": [],
            "error": f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)}"
        }
    except Exception as e:
        return {
            "valid": False, 
            "models": [],
            "error": f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"
        }

# GPT API í‚¤ ì…ë ¥ ë° ê²€ì¦
with st.sidebar:
    st.header("ğŸ¤– GPT API ì„¤ì •")
    api_key = st.text_input(
        "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        type="password",
        help="OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. https://platform.openai.com/api-keys ì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”."
    )
    
    # API í‚¤ ê²€ì¦ ë²„íŠ¼
    if api_key:
        if st.button("ğŸ” API í‚¤ ê²€ì¦", help="ì…ë ¥í•œ API í‚¤ê°€ ìœ íš¨í•œì§€ í™•ì¸í•©ë‹ˆë‹¤"):
            with st.spinner("API í‚¤ë¥¼ ê²€ì¦í•˜ëŠ” ì¤‘..."):
                validation_result = validate_api_key(api_key)
                if validation_result["valid"]:
                    st.success(f"âœ… API í‚¤ê°€ ìœ íš¨í•©ë‹ˆë‹¤!\nì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(validation_result['models'])}")
                else:
                    st.error(f"âŒ API í‚¤ ì˜¤ë¥˜: {validation_result['error']}")
    
    gpt_model = st.selectbox(
        "GPT ëª¨ë¸ ì„ íƒ:",
        ["gpt-4", "gpt-3.5-turbo"],
        index=0,
        help="gpt-4 ëª¨ë¸ì´ ë” ì •í™•í•œ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤."
    )

uploaded_files = st.file_uploader("ì—‘ì…€ íŒŒì¼(.xlsx)ì„ í•˜ë‚˜ ì´ìƒ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx"], accept_multiple_files=True)

def safe_read_excel(file):
    try:
        df = pd.read_excel(file, dtype=str)
        df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)
        return df
    except Exception as e:
        st.error(f"ì—‘ì…€ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def remove_reapplication_rows(df):
    """
    ë°ì´í„°í”„ë ˆì„ì—ì„œ 'ì¬ìš”ì²­'ì´ í¬í•¨ëœ í–‰ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜
    """
    if df is None or df.empty:
        return df
    
    # ëª¨ë“  ì…€ì—ì„œ 'ì¬ìš”ì²­' ë¬¸ìì—´ì´ í¬í•¨ëœ í–‰ì„ ì°¾ì•„ì„œ ì œê±°
    mask = df.astype(str).apply(lambda x: x.str.contains('ì¬ìš”ì²­', na=False)).any(axis=1)
    removed_count = mask.sum()
    
    if removed_count > 0:
        st.info(f"'ì¬ìš”ì²­'ì´ í¬í•¨ëœ {removed_count}ê°œ í–‰ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return df[~mask].reset_index(drop=True)

def classify_university_region(university_name):
    """
    ëŒ€í•™ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì§€ì—­ì„ ë¶„ë¥˜í•˜ëŠ” í•¨ìˆ˜
    """
    university_name = str(university_name).strip()
    
    # ì¸ì„œìš¸ ëŒ€í•™ë“¤
    seoul_universities = [
        "ì„œìš¸ëŒ€", "ì—°ì„¸ëŒ€", "ê³ ë ¤ëŒ€", "ì„±ê· ê´€ëŒ€", "í•œì–‘ëŒ€", "ì¤‘ì•™ëŒ€", "ê²½í¬ëŒ€", "í•œêµ­ì™¸êµ­ì–´ëŒ€", "ì„œê°•ëŒ€", "ì´í™”ì—¬ìëŒ€",
        "í™ìµëŒ€", "ê±´êµ­ëŒ€", "ë™êµ­ëŒ€", "êµ­ë¯¼ëŒ€", "ìˆ­ì‹¤ëŒ€", "ì„¸ì¢…ëŒ€", "ê´‘ìš´ëŒ€", "ëª…ì§€ëŒ€", "ìƒëª…ëŒ€", "ì„œìš¸ì‹œë¦½ëŒ€",
        "ë•ì„±ì—¬ëŒ€", "ì„±ì‹ ì—¬ëŒ€", "ìˆ™ëª…ì—¬ëŒ€", "ë™ë•ì—¬ëŒ€", "ì„œìš¸ì—¬ëŒ€", "í•œì„±ëŒ€", "ì„œê²½ëŒ€", "ê°€í†¨ë¦­ëŒ€", "ì´ì‹ ëŒ€",
        "ì¶”ê³„ì˜ˆìˆ ëŒ€", "í•œêµ­ì²´ìœ¡ëŒ€", "ì„œìš¸êµìœ¡ëŒ€", "ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€", "í•œêµ­ì˜ˆìˆ ì¢…í•©í•™êµ", "ìœ¡êµ°ì‚¬ê´€í•™êµ",
        "ì„œìš¸ê¸°ë…ëŒ€", "ì¥ë¡œíšŒì‹ í•™ëŒ€", "ê°ë¦¬êµì‹ í•™ëŒ€", "í•œì¼ì¥ì‹ ëŒ€", "í˜‘ì„±ëŒ€", "ì„œìš¸í•œì˜ëŒ€", "ì„œìš¸ë””ì§€í„¸ëŒ€"
    ]
    
    # ê²½ê¸°ê¶Œ ëŒ€í•™ë“¤  
    gyeonggi_universities = [
        "ì„±ê· ê´€ëŒ€", "í•œì–‘ëŒ€", "ê²½ê¸°ëŒ€", "ì•„ì£¼ëŒ€", "ì¸í•˜ëŒ€", "ê°€ì²œëŒ€", "ë‹¨êµ­ëŒ€", "ê°•ë‚¨ëŒ€", "ìš©ì¸ëŒ€", "ìˆ˜ì›ëŒ€",
        "í•œì‹ ëŒ€", "í‰íƒëŒ€", "ì„ì§€ëŒ€", "ì°¨ì˜ê³¼í•™ëŒ€", "ëŒ€ì§„ëŒ€", "í•œêµ­ì‚°ì—…ê¸°ìˆ ëŒ€", "ìˆ˜ì›ê³¼í•™ëŒ€", "ê²½ì¸êµìœ¡ëŒ€",
        "í•œê²½ëŒ€", "ì‹ í•œëŒ€", "ì„œìš¸ì‹ í•™ëŒ€", "ì•ˆì–‘ëŒ€", "ë£¨í„°ëŒ€", "ì„œì •ëŒ€", "ê¹€í¬ëŒ€", "ì—¬ì£¼ëŒ€"
    ]
    
    # ì§€ì—­ í‚¤ì›Œë“œë¡œ ë¶„ë¥˜
    if any(univ in university_name for univ in seoul_universities) or "ì„œìš¸" in university_name:
        return "ì¸ì„œìš¸"
    elif any(univ in university_name for univ in gyeonggi_universities) or any(region in university_name for region in ["ê²½ê¸°", "ì¸ì²œ", "ìˆ˜ì›", "ì„±ë‚¨", "ì•ˆì–‘", "ë¶€ì²œ", "ê³ ì–‘", "ìš©ì¸"]):
        return "ê²½ê¸°ê¶Œ"
    elif university_name in ["ë¯¸ê¸°ì¬", "", "NaN", "nan", "None"]:
        return "ë¯¸ê¸°ì¬"
    else:
        return "ì§€ë°©ëŒ€í•™"

def analyze_data_by_region(total_counts):
    """
    ì§€ì—­ë³„ë¡œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    """
    # ì§€ì—­ ë¶„ë¥˜ ì¶”ê°€
    total_counts_with_region = total_counts.copy()
    total_counts_with_region['ì§€ì—­'] = total_counts_with_region['ëŒ€í•™'].apply(classify_university_region)
    
    # ì§€ì—­ë³„ ì§‘ê³„
    region_summary = total_counts_with_region.groupby('ì§€ì—­')['ì§€ì›ìˆ˜'].agg(['sum', 'count']).reset_index()
    region_summary.columns = ['ì§€ì—­', 'ì´_ì§€ì›ìˆ˜', 'ëŒ€í•™_ìˆ˜']
    region_summary['í‰ê· _ì§€ì›ìˆ˜'] = region_summary['ì´_ì§€ì›ìˆ˜'] / region_summary['ëŒ€í•™_ìˆ˜']
    region_summary = region_summary.sort_values('ì´_ì§€ì›ìˆ˜', ascending=False)
    
    return total_counts_with_region, region_summary

def generate_gpt_report(api_key, model, total_counts, region_summary, total_counts_with_region):
    """
    GPT APIë¥¼ ì‚¬ìš©í•´ ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ì¬ì‹œë„ ë° íƒ€ì„ì•„ì›ƒ ê°œì„ )
    """
    max_retries = 3
    timeouts = [60, 90, 120]  # ì ì§„ì ìœ¼ë¡œ íƒ€ì„ì•„ì›ƒ ì¦ê°€
    
    for attempt in range(max_retries):
        try:
            # ë°ì´í„° ì¤€ë¹„
            total_students = int(total_counts['ì§€ì›ìˆ˜'].sum())  # int64ë¥¼ intë¡œ ë³€í™˜
            top_universities = total_counts.head(10)
            
            # ì§€ì—­ë³„ ìƒì„¸ ë°ì´í„° (JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜)
            region_details = {}
            for region in ['ì¸ì„œìš¸', 'ê²½ê¸°ê¶Œ', 'ì§€ë°©ëŒ€í•™']:
                region_data = total_counts_with_region[total_counts_with_region['ì§€ì—­'] == region]
                if not region_data.empty:
                    # pandas íƒ€ì…ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                    region_details[region] = {
                        'ì´_ì§€ì›ìˆ˜': int(region_data['ì§€ì›ìˆ˜'].sum()),
                        'ëŒ€í•™_ìˆ˜': int(len(region_data)),
                        'ìƒìœ„_ëŒ€í•™': [
                            {
                                'ëŒ€í•™': str(row['ëŒ€í•™']),
                                'ì§€ì›ìˆ˜': int(row['ì§€ì›ìˆ˜']),
                                'ì§€ì—­': str(row['ì§€ì—­'])
                            }
                            for _, row in region_data.head(5).iterrows()
                        ]
                    }
            
            # region_summaryë„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜
            region_summary_dict = []
            for _, row in region_summary.iterrows():
                region_summary_dict.append({
                    'ì§€ì—­': str(row['ì§€ì—­']),
                    'ì´_ì§€ì›ìˆ˜': int(row['ì´_ì§€ì›ìˆ˜']),
                    'ëŒ€í•™_ìˆ˜': int(row['ëŒ€í•™_ìˆ˜']),
                    'í‰ê· _ì§€ì›ìˆ˜': float(row['í‰ê· _ì§€ì›ìˆ˜'])
                })
            
            # top_universitiesë„ ë³€í™˜
            top_universities_dict = []
            for _, row in top_universities.iterrows():
                top_universities_dict.append({
                    'ëŒ€í•™': str(row['ëŒ€í•™']),
                    'ì§€ì›ìˆ˜': int(row['ì§€ì›ìˆ˜'])
                })
            
            # ë” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•
            prompt = f"""
ê³ ë“±í•™êµ ëŒ€í•™ ì§€ì› í˜„í™© ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ê¸°ë³¸ í˜„í™©: ì „ì²´ {total_students}ëª…, {len(total_counts)}ê°œ ëŒ€í•™

ì§€ì—­ë³„ í˜„í™©:
{json.dumps(region_summary_dict, ensure_ascii=False)}

ì¸ê¸° ëŒ€í•™ TOP 5:
{json.dumps(top_universities_dict[:5], ensure_ascii=False)}

ë‹¤ìŒ êµ¬ì¡°ë¡œ ê°„ê²°í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

1. **ì „ì²´ í˜„í™© ìš”ì•½** (3-4ì¤„)
2. **ì§€ì—­ë³„ ë¶„ì„** (ê° ì§€ì—­ë³„ 2-3ì¤„)
   - ì¸ì„œìš¸ ëŒ€í•™ íŠ¹ì§•
   - ê²½ê¸°ê¶Œ ëŒ€í•™ íŠ¹ì§•
   - ì§€ë°©ëŒ€í•™ íŠ¹ì§•
3. **ì£¼ìš” ë°œê²¬ì‚¬í•­** (3-4ê°œ í¬ì¸íŠ¸)
4. **ì§„í•™ ì§€ë„ ì œì–¸** (3-4ê°œ ì‹¤ìš©ì  ì¡°ì–¸)

ê° ì„¹ì…˜ì€ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""

            # GPT API í˜¸ì¶œ (ê°œì„ ëœ ì„¤ì •)
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": model,
                "messages": [
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê³ ë“±í•™êµ ì§„í•™ ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 1500,  # í† í° ìˆ˜ ì¤„ì„
                "temperature": 0.3
            }
            
            timeout = timeouts[attempt]
            st.info(f"ğŸ“¡ ì‹œë„ {attempt + 1}/{max_retries}: API ìš”ì²­ ì¤‘... (íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ)")
            
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            elif response.status_code == 429:
                wait_time = 2 ** attempt  # ì§€ìˆ˜ì  ë°±ì˜¤í”„
                st.warning(f"â³ API ì‚¬ìš©ëŸ‰ ì œí•œ. {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(wait_time)
                continue
            else:
                if attempt == max_retries - 1:
                    return f"API ì˜¤ë¥˜: {response.status_code} - {response.text}"
                continue
                
        except requests.exceptions.Timeout:
            if attempt == max_retries - 1:
                return f"â° API ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            st.warning(f"â° íƒ€ì„ì•„ì›ƒ ë°œìƒ. ë‹¤ì‹œ ì‹œë„ ì¤‘... ({attempt + 1}/{max_retries})")
            continue
            
        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:
                return f"ğŸŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)}"
            st.warning(f"ğŸŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜. ë‹¤ì‹œ ì‹œë„ ì¤‘... ({attempt + 1}/{max_retries})")
            continue
            
        except Exception as e:
            return f"ğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"
    
    return "âŒ ëª¨ë“  ì¬ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

def default_col_by_letter(df, letter):
    pos = ord(letter.upper()) - ord('A') + 1
    if 1 <= pos <= len(df.columns):
        return df.columns[pos-1]
    return None

def build_univ_counts_from_series(series: pd.Series) -> pd.DataFrame:
    s = series.astype(str)
    s = s.replace({"": "ë¯¸ê¸°ì¬", "NaN": "ë¯¸ê¸°ì¬", "nan": "ë¯¸ê¸°ì¬", "None": "ë¯¸ê¸°ì¬"}).fillna("ë¯¸ê¸°ì¬")
    s = s.apply(lambda x: x.strip() if isinstance(x, str) else x)
    vc = s.value_counts(dropna=False)
    out = vc.rename_axis("ëŒ€í•™").reset_index(name="ì§€ì›ìˆ˜")
    out = out.sort_values("ì§€ì›ìˆ˜", ascending=False, kind="mergesort").reset_index(drop=True)
    return out

def make_title_from_df(df):
    try:
        c_val = str(df.iloc[0, 2]) if df.shape[1] > 2 else ""
        d_val = str(df.iloc[0, 3]) if df.shape[1] > 3 else ""
        b_val = str(df.iloc[0, 1]) if df.shape[1] > 1 else ""
        base = " ".join([v for v in [c_val, d_val, b_val] if v])
        if base.strip():
            return f"{base} ìˆ˜ì‹œ ì§€ì› ëŒ€í•™ ì‹œê°í™”"
    except Exception:
        pass
    return "ëŒ€í•™ë³„ ì§€ì› ë¹ˆë„ ì‹œê°í™”"

if uploaded_files:
    # ì²« íŒŒì¼ë¡œ ê¸°ë³¸ ì»¬ëŸ¼ ì¶”ì •
    first_df = safe_read_excel(uploaded_files[0])
    if first_df is None or first_df.empty:
        st.warning("ì²« ë²ˆì§¸ íŒŒì¼ì´ ë¹„ì–´ ìˆê±°ë‚˜ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    # ì¬ìš”ì²­ í–‰ ì œê±°
    first_df = remove_reapplication_rows(first_df)
    
    if first_df.empty:
        st.warning("ì¬ìš”ì²­ í–‰ì„ ì œê±°í•œ í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    default_univ_col = default_col_by_letter(first_df, "G") or first_df.columns[0]
    univ_col = st.selectbox(
        "ëŒ€í•™(ë¹ˆë„) ì»¬ëŸ¼ ì„ íƒ (ëª¨ë“  íŒŒì¼ì— ë™ì¼í•˜ê²Œ ì ìš©)",
        options=list(first_df.columns),
        index=(list(first_df.columns).index(default_univ_col) if default_univ_col in first_df.columns else 0),
        help="ë³´í†µ Gì—´(7ë²ˆì§¸ ì—´)ì´ ëŒ€í•™ëª…ì…ë‹ˆë‹¤."
    )

    # ë‹¨ì¼/ë‹¤ì¤‘ì— ë”°ë¥¸ ì œëª©
    if len(uploaded_files) == 1:
        graph_title = make_title_from_df(first_df)
    else:
        graph_title = "ì „ì²´(ë‹¤ì¤‘ íŒŒì¼) ìˆ˜ì‹œ ì§€ì› ëŒ€í•™ ì‹œê°í™”"

    # ëª¨ë“  íŒŒì¼ ë¡œë“œ & í•©ì‚°
    per_file_counts = []   # ê° íŒŒì¼ë³„ ì§‘ê³„ ì €ì¥ (ê²€ì¦ìš©)
    all_univ_values = []   # í•©ì‚°ìš© ì‹œë¦¬ì¦ˆ ëª¨ìŒ
    total_removed_rows = 0  # ì „ì²´ ì œê±°ëœ í–‰ ìˆ˜

    for f in uploaded_files:
        df = safe_read_excel(f)
        if df is None or df.empty:
            st.warning(f"ë¹„ì–´ ìˆê±°ë‚˜ ì½ì„ ìˆ˜ ì—†ëŠ” íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤: {getattr(f, 'name', 'íŒŒì¼')}")
            continue
        
        # ì¬ìš”ì²­ í–‰ ì œê±°
        original_count = len(df)
        df = remove_reapplication_rows(df)
        removed_count = original_count - len(df)
        total_removed_rows += removed_count
        
        if df.empty:
            st.warning(f"ì¬ìš”ì²­ í–‰ ì œê±° í›„ ë°ì´í„°ê°€ ì—†ëŠ” íŒŒì¼: {getattr(f, 'name', 'íŒŒì¼')}")
            continue
            
        if univ_col not in df.columns:
            st.warning(f"ì„ íƒí•œ ì»¬ëŸ¼ '{univ_col}'ì´ ì—†ëŠ” íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤: {getattr(f, 'name', 'íŒŒì¼')}")
            continue

        # í•©ì‚°ì„ ìœ„í•´ ì›ì‹œ ì‹œë¦¬ì¦ˆë§Œ ëª¨ìœ¼ê³ , ê°œë³„ í‘œë„ ìƒì„±
        s = df[univ_col]
        all_univ_values.append(s)
        per_file_counts.append({
            "file": getattr(f, "name", "íŒŒì¼"),
            "counts": build_univ_counts_from_series(s),
            "removed_rows": removed_count
        })

    if not all_univ_values:
        st.error("ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì»¬ëŸ¼ ì„ íƒ ë˜ëŠ” íŒŒì¼ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        st.stop()

    # ì „ì²´ ì œê±°ëœ í–‰ ìˆ˜ í‘œì‹œ
    if total_removed_rows > 0:
        st.success(f"ì´ {total_removed_rows}ê°œì˜ 'ì¬ìš”ì²­' í–‰ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")

    merged_series = pd.concat(all_univ_values, ignore_index=True)
    total_counts = build_univ_counts_from_series(merged_series)

    # ì§€ì—­ë³„ ë¶„ì„ ë°ì´í„° ìƒì„±
    total_counts_with_region, region_summary = analyze_data_by_region(total_counts)

    # ìƒìœ„ Nê°œ ì˜µì…˜
    c1, c2 = st.columns([1, 3])
    with c1:
        top_n = st.number_input("ìƒìœ„ Nê°œë§Œ í‘œì‹œ (0=ì „ì²´)", min_value=0, max_value=int(len(total_counts)), value=min(20, int(len(total_counts))))
    with c2:
        sort_desc = st.checkbox("ë¹ˆë„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬", value=True)

    plot_df = total_counts.copy()
    if sort_desc:
        plot_df = plot_df.sort_values("ì§€ì›ìˆ˜", ascending=False, kind="mergesort")
    if top_n and top_n > 0:
        plot_df = plot_df.head(int(top_n))

    # íŒ”ë ˆíŠ¸ (ë” ì»¬ëŸ¬í’€)
    palette = px.colors.qualitative.Set3 + px.colors.qualitative.Vivid + px.colors.qualitative.Dark24

    # ë§‰ëŒ€ê·¸ë˜í”„ (ì „ì²´ í•©ì‚°)
    fig = px.bar(
        plot_df,
        x="ëŒ€í•™",
        y="ì§€ì›ìˆ˜",
        color="ëŒ€í•™",
        text="ì§€ì›ìˆ˜",
        title=graph_title,
        color_discrete_sequence=palette
    )
    fig.update_traces(textposition="outside")
    fig.update_layout(
        xaxis_tickangle=-45,
        xaxis_title=None,
        yaxis_title=None,
        margin=dict(l=10, r=10, t=60, b=10),
        showlegend=False
    )
    st.plotly_chart(fig, use_container_width=True)

    # ì§€ì—­ë³„ í˜„í™© ê·¸ë˜í”„
    st.subheader("ğŸ“Š ì§€ì—­ë³„ ëŒ€í•™ ì§€ì› í˜„í™©")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ì§€ì—­ë³„ ì´ ì§€ì›ìˆ˜ íŒŒì´ì°¨íŠ¸
        fig_pie = px.pie(
            region_summary,
            values='ì´_ì§€ì›ìˆ˜',
            names='ì§€ì—­',
            title="ì§€ì—­ë³„ ì§€ì› ë¹„ìœ¨",
            color_discrete_sequence=px.colors.qualitative.Set3
        )
        st.plotly_chart(fig_pie, use_container_width=True)
    
    with col2:
        # ì§€ì—­ë³„ ëŒ€í•™ ìˆ˜ ë§‰ëŒ€ê·¸ë˜í”„
        fig_bar = px.bar(
            region_summary,
            x='ì§€ì—­',
            y='ëŒ€í•™_ìˆ˜',
            title="ì§€ì—­ë³„ ëŒ€í•™ ìˆ˜",
            color='ì§€ì—­',
            color_discrete_sequence=px.colors.qualitative.Set3
        )
        st.plotly_chart(fig_bar, use_container_width=True)

    # ì§€ì—­ë³„ ìš”ì•½ í‘œ
    st.subheader("ğŸ“‹ ì§€ì—­ë³„ ìš”ì•½ í†µê³„")
    st.dataframe(region_summary, use_container_width=True)

    # GPT ë¶„ì„ ë³´ê³ ì„œ
    st.subheader("ğŸ¤– AI ë¶„ì„ ë³´ê³ ì„œ")
    
    if api_key:
        # API í‚¤ ìƒíƒœ í‘œì‹œ
        col1, col2 = st.columns([3, 1])
        with col1:
            if st.button("ğŸ“Š AI ë¶„ì„ ë³´ê³ ì„œ ìƒì„±", type="primary"):
                with st.spinner("GPTê°€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ë³´ê³ ì„œë¥¼ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    # API í‚¤ ì¬ê²€ì¦
                    validation = validate_api_key(api_key)
                    if not validation["valid"]:
                        st.error(f"âŒ API í‚¤ ì˜¤ë¥˜: {validation['error']}")
                        st.stop()
                    
                    report = generate_gpt_report(api_key, gpt_model, total_counts, region_summary, total_counts_with_region)
                    
                    if report.startswith("API ì˜¤ë¥˜") or report.startswith("ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜"):
                        st.error(report)
                    else:
                        st.markdown("### ğŸ“„ ëŒ€í•™ ì§€ì› í˜„í™© ë¶„ì„ ë³´ê³ ì„œ")
                        st.markdown(report)
                        
                        # ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                        st.download_button(
                            "ğŸ“ ë³´ê³ ì„œ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
                            data=report.encode("utf-8"),
                            file_name="ëŒ€í•™ì§€ì›í˜„í™©_ë¶„ì„ë³´ê³ ì„œ.txt",
                            mime="text/plain"
                        )
        with col2:
            st.info("ğŸ’¡ íŒ: ë¨¼ì € API í‚¤ë¥¼ ê²€ì¦í•´ë³´ì„¸ìš”!")
    else:
        st.warning("âš ï¸ GPT API í‚¤ë¥¼ ì…ë ¥í•˜ì‹œë©´ AI ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        st.info("ğŸ”— API í‚¤ ë°œê¸‰: https://platform.openai.com/api-keys")

    # ì „ì²´ í•©ì‚° í‘œ & ë‹¤ìš´ë¡œë“œ
    with st.expander("ì „ì²´ í•©ì‚° í‘œ ë³´ê¸°"):
        st.dataframe(total_counts_with_region, use_container_width=True)

    st.download_button(
        "ì „ì²´ í•©ì‚° CSV ë‹¤ìš´ë¡œë“œ (ì§€ì—­ë¶„ë¥˜ í¬í•¨)",
        data=total_counts_with_region.to_csv(index=False).encode("utf-8-sig"),
        file_name="ëŒ€í•™ë³„_ì§€ì›ë¹ˆë„_ì „ì²´í•©ì‚°_ì§€ì—­ë¶„ë¥˜.csv",
        mime="text/csv"
    )

    # (ì„ íƒ) íŒŒì¼ë³„ ì§‘ê³„ë„ í™•ì¸
    with st.expander("íŒŒì¼ë³„ ì§‘ê³„ í‘œ ë³´ê¸°"):
        for item in per_file_counts:
            st.markdown(f"**íŒŒì¼:** {item['file']} (ì¬ìš”ì²­ ì œê±°: {item['removed_rows']}ê°œ í–‰)")
            st.dataframe(item["counts"], use_container_width=True)
            st.markdown("---")
else:
    st.info("ì—‘ì…€ íŒŒì¼ì„ 1ê°œ ì´ìƒ ì—…ë¡œë“œí•˜ë©´ ì „ì²´ í•©ì‚° ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
